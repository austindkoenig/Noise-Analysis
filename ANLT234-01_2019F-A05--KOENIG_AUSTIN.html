<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Austin Koenig" />
  <title>A Brief Noise Analysis</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">A Brief Noise Analysis</h1>
<p class="author">Austin Koenig</p>
</header>
<h1 id="introduction">Introduction</h1>
<p>Noise is the reason why every desired result about nature (i.e.¬†the environment) is not deterministically calculable. Unfortunately, we‚Äôve yet to find a way to avoid it altogether; but, there exist many methods of removing noise from the observed data in order to obtain the signal of importance. Because we can‚Äôt exactly calculate each property of the universe, there is a lot of uncertainty in our daily lives. Fortunately, the modern day brings with it machine learning techniques which can help us learn. However, like our measurement instruments, they are also susceptible to noise.</p>
<p>This is a study on noise and how it affects the performance of machine learning models with it‚Äôs prominence in training data. First, we embark on a brief discussion on noise and it‚Äôs origin; then, we consider some experimental results comparing the tolerance of noise between three different machine learning models.</p>
<h1 id="noise-genesis">Noise Genesis</h1>
<p>As humans, our methods of observation include the systems involving our primary senses. For self-driving cars, these methods of observation may include systems which rely on cameras, LIDAR sensors, and accelerometers. In any case, agents use these methods of observation to measure certain aspects of the environment. The act of an agent observing an environmental aspect is one point of noise genesis. Due to the imprecision of known measurement methods, the observations of the agent are not precise. This lack of precision propagates into the agent‚Äôs internal model of the environment, which thereby induces potentially inappropriate outputs.</p>
<p>There are other sources of noise, but this is the one which we will focus on in this blog post. When noise is present, we often face issues of overfitting data that doesn‚Äôt truly reflect the natural environment. Thus, noise should be avoided at all costs.</p>
<p>Next, let‚Äôs create some data with some artificial noise with which to create a few machine learning algorithms. We will observe which algorithms withstand noise and which will fall to it‚Äôs imprecise effect. Finally, we will discuss some further topics which can be studied in the future.</p>
<h1 id="data">Data</h1>
<p>The data used was generated from a sine wave. We simply sampled points in an interval and generated the sine value using Numpy. A sine wave was chosen because it is a simple function that isn‚Äôt a polynomial. This is an important feature of the experiment because one of our models will employ polynomials, so we don‚Äôt want there to be a ‚Äúcompetitive advantage‚Äù.</p>
<p>Following is a brief description on how each of the sets of data were calculated.</p>
<ul>
<li><p>Training &amp; Testing Inputs:</p>
<p><br /><span class="math display"><em>X</em>‚ÄÑ=‚ÄÑ{<em>x</em>‚ÄÑ‚àà‚ÄÑ‚Ñù[‚ÄÖ‚àí‚ÄÖ2<em>œÄ</em>,‚ÄÜ2<em>œÄ</em>]}</span><br /></p></li>
<li><p>Testing Outputs:</p>
<p><br /><span class="math display"><em>Y</em><sub><em>t</em><em>e</em><em>s</em><em>t</em></sub>‚ÄÑ=‚ÄÑ{sin‚ÄÜ<em>x</em>‚ÄÖ|‚ÄÖ<em>x</em>‚ÄÑ‚àà‚ÄÑ<em>X</em>}</span><br /></p></li>
<li><p>Training Outputs:</p>
<p><br /><span class="math display"><em>Y</em><sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub>‚ÄÑ=‚ÄÑ{ùí©(<em>y</em>,‚ÄÜ<em>œÉ</em>)‚ÄÖ|‚ÄÖ<em>y</em>‚ÄÑ‚àà‚ÄÑ<em>Y</em><sub><em>t</em><em>e</em><em>s</em><em>t</em></sub>,‚ÄÖ<em>œÉ</em>‚ÄÑ‚àà‚ÄÑ<em>Œ£</em>}</span><br /></p>
<p>where <span class="math inline"><em>Œ£</em></span> is the set of all ‚Äúnoise levels‚Äù.</p></li>
</ul>
<p>In English, this means that all input values are real numbers between <span class="math inline">‚ÄÖ‚àí‚ÄÖ2<em>œÄ</em></span> and <span class="math inline">2<em>œÄ</em></span>; the testing outputs are the exact sine value, and the training outputs are samples from the normal distribution with mean equal to the exact sine value and a varying standard deviation. The standard deviation varies across what we will call <em>degrees of noise</em>, which ranges between 0.001 and 1.</p>
<p>The Python package <code>numpy</code> offers a few different ways we can range through the interval <span class="math inline">[0.001,‚ÄÜ1]</span>. We are interested in two of them:</p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html"><code>numpy.linspace</code></a>: Linear spacing means that the points that are sampled are equidistant from each other.</li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html"><code>numpy.geomspace</code></a>: Geometric spacing means that the points that are sampled are equidistant from each other on a log scale.</li>
</ul>
<p>The reason this matters is that if we use geometric (or logarithmic) spacing, then the sampled points will be concentrated in a nonlinear fashion. To illustrate this more clearly, consider the following plot:</p>
<center>
<figure>
<img src="./images/noise.png" style="width:80.0%" alt="" /><figcaption>Figure 1: Noise Plot</figcaption>
</figure>
</center>
<p>Notice that the points sampled linearly create a straight line whereas the points sampled geometrically are concentrated more in the lower values even though they vary across the same interval. If we were concerned more about situations with less noise, perhaps the geometrically spaced samples would suit us; however, we wish to be robust and simply use the linearly spaced points.</p>
<p>Overall, we have two sets: training and testing. Our training set has one set of inputs and 16 sets of outputs (we experiment with 16 degrees of noise). The testing set has one set of inputs and one set of outputs. The goal is to create and test a separate model of each variety described below for each degree of noise. Then, we will compare the errors and predictions of each type of model to see how they withstand the noise that we‚Äôve just generated.</p>
<h1 id="models">Models</h1>
<p>We are comparing the following models to see which will be the least sensitive to noise:</p>
<ul>
<li><strong>Polynomial Regressor (PNR)</strong> - A polynomial regressor is an extension of linear regression in that it employs higher order terms to make predictions. In particular, we‚Äôll use a tenth degree polynomial.</li>
<li><strong>Gradient Boost Regressor (GBR)</strong> - A gradient boost regressor is an extension of decision trees (or random forests) in that it uses gradient descent for parameter optimization.</li>
<li><strong>Artificial Neural Network (ANN)</strong> - Shallow neural networks can be considered as functions represented by only an input layer, an output layer, and one hidden layer between them.</li>
<li><strong>Deep Neural Network (DNN)</strong> - Deep neural networks are similar to shallow neural networks; they simply contain more than one hidden layer.</li>
</ul>
<p><a href="#appendix-a">Appendix A</a> contains the particular structures of the models that were trained.</p>
<p>This group of models was chosen because it includes one model that doesn‚Äôt use gradient descent, two models which are relatively small by number of parameters, and one deep neural network that is presumably the most accurate model. This mix seems to encapsulate a fair amount of the varieties of learning models that are employed. How did these models actually perform?</p>
<h1 id="results-discussion">Results &amp; Discussion</h1>
<p>Let‚Äôs examine the errors of each model with respect to the amount of noise added.</p>
<center>
<figure>
<img src="./images/errors.png" style="width:80.0%" alt="" /><figcaption>Figure 2: Error Plot</figcaption>
</figure>
</center>
<p>While knowing the error rating per degree of noise is useful in giving a rating system for the models, it is still very helpful to see the prediction plots for each model. For instance, some techniques operate very poorly around the edges of the domain. This is not easily reflected in the error plot, so we can use them in conjunction with the prediction plots to see in which areas of the domain our models performed the best/worst. Moreover, we can see how these behaviors change as we introduce more and more noise into the data.</p>
<center>
<figure>
<img src="./images/predictions.png" style="width:80.0%" alt="" /><figcaption>Figure 3: Prediction Plot</figcaption>
</figure>
</center>
<p>For the remainder of the blog, we will refer to each model as their abbreviated version above.</p>
<p>Observing the smallest degree of noise, we see that ANN and DNN have a hard time around the edges of the domain whereas GBR and PNR are nearly perfect with this small amount of noise.</p>
<p>As we get to the sixth degree of noise, ANN and DNN are roughly suffering the consequence of not even picking up on the curve to the far right. GBR seems to spread quite a bit around the true sine, though not straying very far from it. This seems to be a symptom of overfitting. The tenth degree PNR is still nearly perfect.</p>
<p>With twelve degrees of noise, GBR overfits even more and has large error near the boundaries. The neural networks maintain their large error in the large positives and DNN seems to be overfitting a small amount as well. PNR remains the best model, but is starting to suffer near the boundaries.</p>
<p>These trends continue into the highest levels of noise as all models lose accuracy, usually first around the boundaries. The neural networks sustained their ‚Äúflatness‚Äù in the large positives throughout all levels of noise while also feeling the effects of overfitting. GBR started overfitting very quickly, but interestingly maintained the general shape of a sine wave throughout all of the noise, which is indicative of some success. PNR loses it‚Äôs shape slightly in the highest levels of noise.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We tested four models against sixteen different degrees of noise and found that GBR and PNR performed the best for our test case. It should be considered, however, that we picked a very particular problem that is unusual in the real world. The entire experiment was completely artificial on purpose because it is a first step towards a more rigorous study of machine learning methods and away from empirical experiments containing unintended, unnoticed, or uncontrolled bias in the data used. Of course, we want to use real-world data in the end, but we should study the algorithms themselves to learn where they will be the most effective.</p>
<p>There are many methods to preprocess data to sift the noise before modeling, none of which we used here. This post was focused mainly on how different types of machine learning algorithms withstand the burden of noise. There are techniques to filter data in nearly every stage of the data science process. We have only observed the capabilities of models in handling the noise themselves. In reality, much more data cleaning and preprocessing would have occured, but much of that was omitted due to the nature of the experiment.</p>
<p>Going forward, we can look at other filtering methods that are not embedded directly into the model itself. Also, a wider range of models and model sizes should be tested. There is even potential for a search problem in finding the best combination of noise reduction techniques using a machine learning model. However, this blog is a good start and poses a platform on which to build even deeper ideas about dealing with noise in data. We have simply withstood it, but in the future we wish to reduce it before it even reaches the models.</p>
<h1 id="references">References</h1>
<p>[1] <a href="https://www.famsf.org/files/Fact%20Sheet_%20birds_of_paradise.pdf">Organism Fact Sheet: Birds of Paradise</a></p>
<h1 id="appendix-a">Appendix A</h1>
<p>This section contains the structures for the models used in the study.</p>
<pre><code>Artificial Neural Network
Model: &quot;model_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              2048      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 1025      
=================================================================
Total params: 3,073
Trainable params: 3,073
Non-trainable params: 0
_________________________________________________________________

Deep Neural Network
Model: &quot;model_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_3 (Dense)              (None, 1024)              2048      
_________________________________________________________________
dense_4 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dense_5 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 1025      
=================================================================
Total params: 2,102,273
Trainable params: 2,102,273
Non-trainable params: 0
_________________________________________________________________

Gradient Boost Regressor
{
  &quot;alpha&quot;: 0.9,
  &quot;criterion&quot;: &quot;friedman_mse&quot;,
  &quot;init&quot;: null,
  &quot;learning_rate&quot;: 0.1,
  &quot;loss&quot;: &quot;ls&quot;,
  &quot;max_depth&quot;: 5,
  &quot;max_features&quot;: null,
  &quot;max_leaf_nodes&quot;: null,
  &quot;min_impurity_decrease&quot;: 0.0,
  &quot;min_impurity_split&quot;: null,
  &quot;min_samples_leaf&quot;: 1,
  &quot;min_samples_split&quot;: 2,
  &quot;min_weight_fraction_leaf&quot;: 0.0,
  &quot;n_estimators&quot;: 100,
  &quot;n_iter_no_change&quot;: null,
  &quot;presort&quot;: &quot;auto&quot;,
  &quot;random_state&quot;: null,
  &quot;subsample&quot;: 1.0,
  &quot;tol&quot;: 0.0001,
  &quot;validation_fraction&quot;: 0.3,
  &quot;verbose&quot;: 1,
  &quot;warm_start&quot;: false
}

Polynomial Regressor (degree: 10)
{
  &quot;copy_X&quot;: true,
  &quot;fit_intercept&quot;: true,
  &quot;n_jobs&quot;: null,
  &quot;normalize&quot;: false
}</code></pre>
<h1 id="appendix-before">Appendix Before</h1>
<p>The code for this project and all files included can be found here: https://github.com/austindkoenig/Noise-Analysis</p>
</body>
</html>
