<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Austin Koenig" />
  <title>A Brief Noise Analysis</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">A Brief Noise Analysis</h1>
<p class="author">Austin Koenig</p>
</header>
<h1 id="introduction">Introduction</h1>
<p>Noise is the reason why every desired result about nature (i.e.¬†the environment) is not deterministically calculable. Unfortunately, we‚Äôve yet to find a way to avoid it altogether; but, there exist many methods of removing noise from the observed data in order to obtain the signal of importance.</p>
<p>This is a study on noise and how it affects the performance of machine learning models with it‚Äôs prominence in training data. First, we embark on a brief discussion on noise and it‚Äôs origin; then, we consider some experimental results comparing the tolerance of noise between three different machine learning models.</p>
<h1 id="noise-genesis">Noise Genesis</h1>
<p>Before we define noise, let‚Äôs prepare a platform off of which to jump towards the idea of noise in observation.</p>
<p>First, let‚Äôs assume our realm of conversation consists only of an agent (e.g.¬†an engineer, robot, self-driving car, etc.) and the environment. Perhaps this environment contains other agents, or perhaps it contains some dynamic processes which allow motion through time and space. In either case, we are only concerned with one agent within an environment.</p>
<p>Furthermore, let‚Äôs describe an agent as an actor with a set of sensory inputs (e.g.¬†eyes, a microphone, an infrared sensor, etc.) and a set of potential control outputs (e.g.¬†raise arm, move forward ten meters, deploy parachute, etc.).</p>
<p>In order for this agent to interact with the environment, some other conditions must be met. Let‚Äôs formally define an agent as follows.</p>
<p>An agent is an actor (e.g.¬†human, robot, car, etc.) that possesses - A set of sensory inputs - A set of control outputs</p>
<p>Moreover, in order for agency to exist, the following conditions must hold: 1. The agent maintains an internal model<sup>1</sup> of the environment. 2. The agent has at least one method of observation. 3. The agent is able to combine environmental observations with the internal environmental model.</p>
<p>Since we care to analyze the effect of noise in certain outcomes, this blog post is primarily concerned with condition 2 and somewhat concerned with condition 3. As humans, our methods of observation include the systems involving our primary senses. For self-driving cars, these methods of observation may include systems which rely on cameras, LIDAR sensors, and accelerometers. In any case, agents use these methods of observation to measure certain aspects of the environment. Intuitively, the aspects which an agent measures are chosen based on learned ideas from previous observations. From this, we assume that the agent has no prior knowledge of the environment such that they have to learn it from pure observation.</p>
<p>The act of an agent observing an environmental aspect is one point of noise genesis. Due to the imprecision of known measurement methods, the observations of the agent are not precise. This lack of precision propagates into the agent‚Äôs internal model of the environment, which thereby induces potentially inappropriate outputs.</p>
<p>Another point of noise genesis comes from bias, which is more of a structure problem than a precision problem. Bias occurs when one observation does not reflect the true nature of the global environment. For instance, birds of paradise in New Guinea<sup>2</sup> are losing their homes due to the biased observations of humans gathering food and building materials.</p>
<p>Why is noise a problem? Noise causes incorrect conclusions to be drawn from data. If we measure city air pollutants with imprecise instruments or in outlier areas, then the noise in the data will be reflected in the decisions made based on those results.</p>
<p>For this reason, it is important to combat noise to the fullest of our abilities. One source of structural noise is a disparity between the perspectives of different agents. Later in this post, we will discuss an experiment comparing a few different machine learning models and their abilities to find the signal in the noise. First, we will see how the data was generated and then briefly describe the models which were tested. Finally, we will discuss the results of the experiment.</p>
<p><sup>1</sup> This usage of the term <em>model</em> is different than the common usage in machine learning. Here, it refers to a general description outlining the components of the environment and how they all work.</p>
<p><sup>2</sup> I highly recommend the Netflix documentary called <a href="https://www.netflix.com/title/80186796">Dancing With The Birds</a>.</p>
<h1 id="data">Data</h1>
<p>The data used was generated from a sine wave. A sine wave was chosen because it is a simple function that isn‚Äôt a polynomial. This is an important feature of the experiment because one of our models will employ polynomials, so we don‚Äôt want there to be a ‚Äúcompetitive advantage‚Äù.</p>
<p>Following is a brief description on how each of the sets of data were calculated.</p>
<ul>
<li><p>Training &amp; Testing Inputs:</p>
<p><br /><span class="math display"><em>X</em>‚ÄÑ=‚ÄÑ{<em>x</em>‚ÄÑ‚àà‚ÄÑ‚Ñù[‚ÄÖ‚àí‚ÄÖ2<em>œÄ</em>,‚ÄÜ2<em>œÄ</em>]}</span><br /></p></li>
<li><p>Testing Outputs:</p>
<p><br /><span class="math display"><em>Y</em><sub><em>t</em><em>e</em><em>s</em><em>t</em></sub>‚ÄÑ=‚ÄÑ{sin‚ÄÜ<em>x</em>‚ÄÖ|‚ÄÖ<em>x</em>‚ÄÑ‚àà‚ÄÑ<em>X</em>}</span><br /></p></li>
<li><p>Training Outputs:</p>
<p><br /><span class="math display"><em>Y</em><sub><em>t</em><em>r</em><em>a</em><em>i</em><em>n</em></sub>‚ÄÑ=‚ÄÑ{ùí©(<em>y</em>,‚ÄÜ<em>œÉ</em><sup>2</sup>)‚ÄÖ|‚ÄÖ<em>y</em>‚ÄÑ‚àà‚ÄÑ<em>Y</em><sub><em>t</em><em>e</em><em>s</em><em>t</em></sub>,‚ÄÖ<em>œÉ</em><sup>2</sup>‚ÄÑ‚àà‚ÄÑ<em>Œ£</em>}</span><br /></p>
<p>where <span class="math inline"><em>Œ£</em></span> is the set of all ‚Äúnoise levels‚Äù.</p></li>
</ul>
<p>In English, this means that all input values are real numbers between <span class="math inline">‚ÄÖ‚àí‚ÄÖ2<em>œÄ</em></span> and <span class="math inline">2<em>œÄ</em></span>; the testing outputs are the exact sine value, and the training outputs are samples from the normal distribution with mean equal to the exact sine value and a varying standard deviation. The standard deviation varies across what we will call <em>degrees of noise</em>, which ranges between 0.001 and 1.</p>
<p>The Python package <code>numpy</code> offers a few different ways we can range through the interval <span class="math inline">[0.001,‚ÄÜ1]</span>. We are interested in two of them:</p>
<ul>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html"><code>numpy.linspace</code></a>: Linear spacing means that the points that are sampled are equidistant from each other.</li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html"><code>numpy.geomspace</code></a>: Geometric spacing means that the points that are sampled are equidistant from each other on a log scale.</li>
</ul>
<p>The reason this matters is that if we use geometric (or logarithmic) spacing, then the sampled points will be concentrated in a nonlinear fashion. To illustrate this more clearly, consider the following plot:</p>
<center>
<figure>
<img src="./images/noise.png" style="width:80.0%" alt="" /><figcaption>Noise Plot</figcaption>
</figure>
</center>
<p>Notice that the points sampled linearly create a straight line whereas the points sampled geometrically are concentrated more in the lower values even though they vary across the same interval. If we were concerned more about situations with less noise, perhaps the geometrically spaced samples would suit us; however, we wish to be robust and simply use the linearly spaced points.</p>
<p>Overall, we have two sets: training and testing. Our training set has one set of inputs and 16 sets of outputs (we experiment with 16 degrees of noise). The testing set has one set of inputs and one set of outputs. The goal is to create and test a separate model of each variety described below for each degree of noise. Then, we will compare the errors and predictions of each type of model to see how they withstand the noise that we‚Äôve just generated.</p>
<h1 id="models">Models</h1>
<p>We are comparing the following models to see which will be the least sensitive to noise:</p>
<ul>
<li><strong>Polynomial Regressor (PNR)</strong> - A polynomial regressor is an extension of linear regression in that it employs higher order terms to make predictions. In particular, we‚Äôll use a tenth degree polynomial.</li>
<li><strong>Gradient Boost Regressor (GBR)</strong> - A gradient boost regressor is an extension of decision trees (or random forests) in that it uses gradient descent for parameter optimization.</li>
<li><strong>Artificial Neural Network (ANN)</strong> - Shallow neural networks can be considered as functions represented by only an input layer, an output layer, and one hidden layer between them.</li>
<li><strong>Deep Neural Network (DNN)</strong> - Deep neural networks are similar to shallow neural networks; they simply contain more than one hidden layer.</li>
</ul>
<p><a href="#appendix-a">Appendix A</a> contains the particular structures of the models that were trained.</p>
<p>This group of models was chosen because it includes one model that doesn‚Äôt use gradient descent, two models which are relatively small by number of parameters, and one deep neural network that is presumably the most accurate model. This mix seems to encapsulate a fair amount of the varieties of learning models that are employed. How did these models actually perform?</p>
<h1 id="results-discussion">Results &amp; Discussion</h1>
<p>Next, we want to look at the errors. Since the models we picked are rather different in the number of parameters, there will be an inherent difference in errors; although, we wish only to observe the trends with respect to one another. Therefore, they have been scaled to a standard normal distribution so that we can observe them all side-by-side.</p>
<center>
<figure>
<img src="./images/errors.png" style="width:80.0%" alt="" /><figcaption>Error Plot</figcaption>
</figure>
</center>
<p>While knowing the error rating per degree of noise is useful in giving a rating system for the models, it is still very helpful to see the prediction plots for each model. For instance, techniques like high-order Newton-Cotes interpolation operate very poorly around the edges of the interval. This is not reflected in the error, so we can use the prediction plots to see in which areas of the domain our models performed the best/worst. Moreover, we can see how these behaviors change as we introduce more and more noise into the data.</p>
<center>
<figure>
<img src="./images/predictions.png" style="width:80.0%" alt="" /><figcaption>Prediction Plot</figcaption>
</figure>
</center>
<p>For the remainder of the blog, we will refer to each model as their abbreviated version above.</p>
<p>Observing the smallest degree of noise, we see that ANN and DNN have a hard time around the edges of the domain whereas GBR and PNR are nearly perfect with this small amount of noise.</p>
<p>As we get to the sixth degree of noise, ANN and DNN are roughly suffering the consequence of not even picking up on the curve to the far right. GBR seems to spread quite a bit around the true sine, though not straying very far from it. This seems to be a symptom of overfitting. The tenth degree PNR is still nearly perfect.</p>
<p>With twelve degrees of noise, GBR overfits even more and has large error near the boundaries. The neural networks maintain their large error in the large positives and DNN seems to be overfitting a small amount as well. PNR remains the best model, but is starting to suffer near the boundaries.</p>
<p>These trends continue into the highest levels of noise as all models lose accuracy, usually first around the boundaries. The neural networks sustained their ‚Äúflatness‚Äù in the large positives throughout all levels of noise while also feeling the effects of overfitting. GBR started overfitting very quickly, but interestingly maintained the general shape of a sine wave throughout all of the noise, which is indicative of some success. PNR loses it‚Äôs shape slightly in the highest levels of noise.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We tested four models against sixteen different degrees of noise and found that GBR and PNR performed the best for our test case. It should be considered, however, that we picked a very particular problem that is unusual in the real world. The entire experiment was completely artificial on purpose because it is a first step towards a more rigorous study of machine learning methods and away from empirical experiments containing unintended, unnoticed, or uncontrolled bias in the data used. Of course, we want to use real-world data in the end, but we should study the algorithms themselves to learn where they will be the most effective.</p>
<p>There are many methods to preprocess data to sift the noise before modeling, none of which we used here. This post was focused mainly on how different types of machine learning algorithms withstand the burden of noise. There are techniques to filter data in nearly every stage of the data science process. We have only observed the capabilities of models in handling the noise themselves. In reality, much more data cleaning and preprocessing would have occured, but much of that was omitted due to the nature of the experiment.</p>
<p>Going forward, we can look at other filtering methods that are not embedded directly into the model itself. Also, a wider range of models and model sizes should be tested. There is even potential for a search problem in finding the best combination of noise reduction techniques using a machine learning model. However, this blog is a good start and poses a platform on which to build even deeper ideas about dealing with noise in data. We have simply withstood it, but in the future we wish to reduce it before it even reaches the models.</p>
<h1 id="references">References</h1>
<p>[1] <a href="https://www.famsf.org/files/Fact%20Sheet_%20birds_of_paradise.pdf">Organism Fact Sheet: Birds of Paradise</a></p>
<h1 id="appendix-a">Appendix A</h1>
<p>This section contains the structures for the models used in the study.</p>
<pre><code>Artificial Neural Network
Model: &quot;model_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_1 (Dense)              (None, 1024)              2048      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 1025      
=================================================================
Total params: 3,073
Trainable params: 3,073
Non-trainable params: 0
_________________________________________________________________

Deep Neural Network
Model: &quot;model_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         (None, 1)                 0         
_________________________________________________________________
dense_3 (Dense)              (None, 1024)              2048      
_________________________________________________________________
dense_4 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dense_5 (Dense)              (None, 1024)              1049600   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 1025      
=================================================================
Total params: 2,102,273
Trainable params: 2,102,273
Non-trainable params: 0
_________________________________________________________________

Gradient Boost Regressor
{
  &quot;alpha&quot;: 0.9,
  &quot;criterion&quot;: &quot;friedman_mse&quot;,
  &quot;init&quot;: null,
  &quot;learning_rate&quot;: 0.1,
  &quot;loss&quot;: &quot;ls&quot;,
  &quot;max_depth&quot;: 5,
  &quot;max_features&quot;: null,
  &quot;max_leaf_nodes&quot;: null,
  &quot;min_impurity_decrease&quot;: 0.0,
  &quot;min_impurity_split&quot;: null,
  &quot;min_samples_leaf&quot;: 1,
  &quot;min_samples_split&quot;: 2,
  &quot;min_weight_fraction_leaf&quot;: 0.0,
  &quot;n_estimators&quot;: 100,
  &quot;n_iter_no_change&quot;: null,
  &quot;presort&quot;: &quot;auto&quot;,
  &quot;random_state&quot;: null,
  &quot;subsample&quot;: 1.0,
  &quot;tol&quot;: 0.0001,
  &quot;validation_fraction&quot;: 0.3,
  &quot;verbose&quot;: 1,
  &quot;warm_start&quot;: false
}

Polynomial Regressor (degree: 10)
{
  &quot;copy_X&quot;: true,
  &quot;fit_intercept&quot;: true,
  &quot;n_jobs&quot;: null,
  &quot;normalize&quot;: false
}</code></pre>
</body>
</html>
